{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    # \"facebook/opt-2.7b\",\n",
    "    # \"meta-math/MetaMath-Mistral-7B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # \"EleutherAI/gpt-j-6B\",\n",
    "    \"upaya07/Arithmo2-Mistral-7B\",\n",
    "    # 'lmsys/vicuna-7b-v1.5',\n",
    "    # \"mistralai/Mistral-7B-v0.1\",\n",
    "    # \"/home/users/nus/e0672129/scratch/MCTS-DPO/sft/diymistral-arithmo-lowerlr/steps25209\",\n",
    "    model_max_length=640,\n",
    "    padding_side='left',\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False,\n",
    "    use_auth_token=\"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\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer('''QUESTION: What do people use to absorb extra ink from a fountain pen?\\nAnswer Choices:\\n(A) shirt pocket\\n(B) calligrapher's hand\\n(C) inkwell\\n(D) desk drawer\\n(E) blotter\\n\\nANSWER: The answer must be used to absorb extra ink.\\nBlotters are designed to absorb liquids.\\nTherefore, the answer is (E) blotter.\\n\\nQUESTION: What home entertainment equipment requires cable?\\nAnswer Choices:\\n(A) radio shack\\n(B) substation\\n(C) television\\n(D) cabinet\\n(E) desk\\n\\nANSWER: The answer must require cable.\\nCable is used to provide satellite channels to televisions.\\nTherefore, the answer is (C) television.\\n\\nQUESTION: Sammy wanted to go to where the people were. Where might he go?\\nAnswer Choices:\\n(A) populated areas\\n(B) race track\\n(C) desert\\n(D) apartment\\n(E) roadblock\\n\\nANSWER: The answer must be a place with many people.\\nPopulated areas, by definition, have a lot of people.\\nTherefore, the answer is (A) populated areas.\\n\\nQUESTION: The fox walked from the city into the forest, what was it looking for?\\nAnswer Choices:\\n(A) pretty flowers\\n(B) hen house\\n(C) natural habitat\\n(D) storybook\\n(E) dense forest\\n\\nANSWER: The answer must be a reason for a fox to go into the forest.\\nThe forest is a fox's natural habitat.\\nTherefore, the answer is (C) natural habitat.\\n\\nQUESTION: Where do you put your grapes just before checking out?\\nAnswer Choices:\\n(A) mouth\\n(B) grocery cart\\n(C) super market\\n(D) fruit basket\\n(E) fruit market\\n\\nANSWER: The answer should be the place where grocery items are placed before checking out.\\nOf the above choices, grocery cart makes the most sense for holding grocery items.\\nTherefore, the answer is (B).\\n\\nQUESTION: Where could you find a seat that sometimes vibrates?\\nAnswer Choices:\\n(A) mall\\n(B) in cinema\\n(C) airplane\\n(D) martorell\\n(E) auditorium\\n\\nANSWER:''',\n",
    "                return_tensors='pt',)['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [198, 198, 35780, 2849, 25], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('''\\n\\nQUESTION:''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n('"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([198, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([35780,  2849,    25,  1867,   466,   661,   779,   284, 17565,  3131]),\n",
       " tensor([  198, 15037, 45532,    25,   383,  3280,   815,   307,   262,  1295,\n",
       "           810, 16918,  3709,   389,  4624,   878, 10627,   503,    13,   198,\n",
       "          5189,   262,  2029,  7747,    11, 16918,  6383,  1838,   262,   749,\n",
       "          2565,   329,  4769, 16918,  3709,    13,   198, 26583,    11,   262,\n",
       "          3280,   318,   357,    33,   737,   198,   198, 35780,  2849,    25,\n",
       "          6350,   714,   345,  1064,   257,  5852,   326,  3360, 12611,   689,\n",
       "            30,   198, 33706, 10031,  1063,    25,   198,     7,    32,     8,\n",
       "         17374,   198,     7,    33,     8,   287, 22041,   198,     7,    34,\n",
       "             8, 19401,   198,     7,    35,     8, 11277,   382,   297,   198,\n",
       "             7,    36,     8, 30625,  1505,   198,   198, 15037, 45532,    25]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[:10], ids[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_TOKEN_IDS = [35780, 2849, 25]\n",
    "\n",
    "def get_final_qa_index(input_ids):\n",
    "    question_indexes = input_ids.eq(QUESTION_TOKEN_IDS[-1]).nonzero()\n",
    "    question_indexes = [_ for _ in question_indexes]\n",
    "    question_index = question_indexes[-1] - (len(QUESTION_TOKEN_IDS) - 1)\n",
    "    question_indexes = question_indexes[::-1]\n",
    "    for idx in question_indexes:\n",
    "        if input_ids[idx - (len(QUESTION_TOKEN_IDS) - 1): idx + 1].tolist() == QUESTION_TOKEN_IDS:\n",
    "            question_index = idx - (len(QUESTION_TOKEN_IDS) - 1)\n",
    "            break\n",
    "    return question_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_qa_index(ids).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QUESTION: Where could you find a seat that sometimes vibrates?\\nAnswer Choices:\\n(A) mall\\n(B) in cinema\\n(C) airplane\\n(D) martorell\\n(E) auditorium\\n\\nANSWER:'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids[461:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0, 513]), tensor([0, 2]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes[::-1][0], indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 18)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexes[::-1]), len(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QUESTION:'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([35780, 2849, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>', '</s>', '</s>', '<unk>')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token, tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How much less does Andy weigh now than at the beginning of the year?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The horizontal asymptote of $f$ is the horizontal line that $f$ approaches as $x \to \\pm \\infty$. When the leading terms of the numerator and denominator have the same degree, that line is at the value equal to the ratio of the leading coefficients, namely $y = 2/1 = 2$.\n",
      "-\n",
      "-Setting this equal to $f(x)$,-\n",
      "-Setting this equal to $f(x)$,-\n"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_utils import PaddingStrategy, TruncationStrategy\n",
    "\n",
    "prompt = (\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    \"How much less does Andy weigh now than at the beginning of the year?\"\n",
    "    \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    \"The horizontal asymptote of $f$ is the horizontal line that $f$ approaches as $x \\to \\pm \\infty$. \"\n",
    "    \"When the leading terms of the numerator and denominator have the same degree, \"\n",
    "    \"that line is at the value equal to the ratio of the leading coefficients, \"\"namely $y = 2/1 = 2$.\\n\"\n",
    ")\n",
    "# prompt = (\n",
    "#     \"QUESTION: \"\n",
    "#     \"Find the distance between the planes $x - 3y + 3z = 8$ and $2x - 6y + 6z = 2.$\\n\\n\"\n",
    "#     \"ANSWER:\"\n",
    "#     \" To find the distance between two parallel planes, we can take any point on one plane and find its perpendicular distance to the other plane.\"\n",
    "# )\n",
    "prompt_ids = tokenizer(\n",
    "    prompt,\n",
    "    add_special_tokens=True,\n",
    "    padding=PaddingStrategy.DO_NOT_PAD,\n",
    "    truncation=TruncationStrategy.LONGEST_FIRST,\n",
    "    return_tensors='pt',\n",
    ")['input_ids'][0]\n",
    "myprompt = tokenizer.decode(prompt_ids, \n",
    "                            clean_up_tokenization_spaces=False,\n",
    "                            skip_special_tokens=False)\n",
    "\n",
    "text = (\n",
    "    \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    \"How much less does Andy weigh now than at the beginning of the year?\"\n",
    "    \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    \"The horizontal asymptote of $f$ is the horizontal line that $f$ approaches as $x \\to \\pm \\infty$. \"\n",
    "    \"When the leading terms of the numerator and denominator have the same degree, \"\n",
    "    \"that line is at the value equal to the ratio of the leading coefficients, \"\"namely $y = 2/1 = 2$.\\n\"\n",
    "    \"Setting this equal to $f(x)$,\"\n",
    ")\n",
    "# text = (\n",
    "#     \"<s>QUESTION: \"\n",
    "#     \"Find the distance between the planes $x - 3y + 3z = 8$ and $2x - 6y + 6z = 2.$\\n\\n\"\n",
    "#     \"ANSWER:\"\n",
    "#     \" To find the distance between two parallel planes, we can take any point on one plane and find its perpendicular distance to the other plane.\"\n",
    "#     \" The answer is 36\"\n",
    "# )\n",
    "\n",
    "# generated = text[len(myprompt):]\n",
    "generated = tokenizer.decode(tokenizer.encode(text)[prompt_ids.size(-1):], \n",
    "                             clean_up_tokenization_spaces=False,\n",
    "                             skip_special_tokens=False)\n",
    "generated = \"Setting this equal to $f(x)$,\"\n",
    "# generated = \" The answer is 36\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    myprompt + generated,\n",
    "    add_special_tokens=False,\n",
    "    truncation=TruncationStrategy.LONGEST_FIRST,\n",
    "    return_tensors='pt',\n",
    ")['input_ids'][0]\n",
    "\n",
    "gen_ids = input_ids[prompt_ids.size(-1):]\n",
    "\n",
    "print('-{}-'.format(myprompt))\n",
    "print('-{}-'.format(generated))\n",
    "print('-{}-'.format(tokenizer.decode(gen_ids,\n",
    "                                     clean_up_tokenization_spaces=False,\n",
    "                                     skip_special_tokens=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   16,   284,   220,    17, 13244])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   14,    16,   284,   220,    17, 13244, 20638,   420,  6273,   311,\n",
       "          400,    69,  2120, 15437,    11])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How much less does Andy weigh now than at the beginning of the year?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The horizontal asymptote of $f$ is the horizontal line that $f$ approaches as $x \to \\pm \\infty$. When the leading terms of the numerator and denominator have the same degree, that line is at the value equal to the ratio of the leading coefficients, namely $y = 2/1 = 2$..\n",
      "Setting this equal to $f(x)$,-\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('-{}-'.format(tokenizer.decode(torch.cat([prompt_ids, gen_ids], dim=-1),\n",
    "                                     clean_up_tokenization_spaces=False,\n",
    "                                     skip_special_tokens=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 28705, 1186, 12171, 1702, 28747, 1602, 1188, 2108, 1235, 16514, 19929, 1055, 821, 438, 272, 5398, 302, 272, 879, 28804, 13, 13, 1251, 9854, 725, 28747, 16514, 478, 24776, 28705, 28740, 28782, 28784, 648, 28705, 28770, 28784, 327, 2087, 28740, 28782, 28784, 28806, 28770, 28784, 28746, 28740, 28774, 28750, 4060, 28740, 28774, 28750, 14090, 1024, 6485, 261, 7590, 28723, 13\n",
      "1, 28705, 1186, 12171, 1702, 28747, 1602, 1188, 2108, 1235, 16514, 19929, 1055, 821, 438, 272, 5398, 302, 272, 879, 28804, 13, 13, 1251, 9854, 725, 28747, 16514, 478, 24776, 28705, 28740, 28782, 28784, 648, 28705, 28770, 28784, 327, 2087, 28740, 28782, 28784, 28806, 28770, 28784, 28746, 28740, 28774, 28750, 4060, 28740, 28774, 28750, 14090, 1024, 6485, 261, 7590, 28723, 13\n",
      "\n",
      "-<s> QUESTION: How much less does Andy weigh now than at the beginning of the year?\n",
      "\n",
      "ANSWER: Andy weighed 156 + 36 = <<156+36=192>>192 pounds after growing taller.\n",
      "-\n",
      "-<s> QUESTION: How much less does Andy weigh now than at the beginning of the year?\n",
      "\n",
      "ANSWER: Andy weighed 156 + 36 = <<156+36=192>>192 pounds after growing taller.\n",
      "-\n",
      "- Andy weighed 156 + 36 = <<156+36=192>>192 pounds after growing taller.\n",
      "-\n",
      "-Andy weighed 156 + 36 = <<156+36=192>>192 pounds after growing taller.\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "print(', '.join([str(x) for x in input_ids]))\n",
    "print(', '.join([str(x) for x in _input_ids]))\n",
    "print('')\n",
    "print('-{}-\\n-{}-\\n-{}-\\n-{}-'.format(text, input_text, output_text, ootxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-{}-'.format(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 25\n",
      "1186, 12171, 1702, 28747, 1602, 1188, 2108, 1235, 16514, 19929, 1055, 821, 438, 272, 5398, 302, 272, 879, 28804, 13, 13, 1251, 9854, 725, 28747\n",
      "1186, 12171, 1702, 28747, 1602, 1188, 2108, 1235, 16514, 19929, 1055, 821, 438, 272, 5398, 302, 272, 879, 28804, 13, 13, 1251, 9854, 725, 28747\n",
      "16514, 478, 24776, 28705, 28740, 28782, 28784, 648, 28705, 28770, 28784, 327, 2087, 28740, 28782, 28784, 28806, 28770, 28784, 28746, 28740, 28774, 28750, 4060, 28740, 28774, 28750, 14090, 1024, 6485, 261, 7590, 28723, 13, 1014, 4372, 349, 28705, 28770, 28784\n",
      "28705, 16514, 478, 24776, 28705, 28740, 28782, 28784, 648, 28705, 28770, 28784, 327, 2087, 28740, 28782, 28784, 28806, 28770, 28784, 28746, 28740, 28774, 28750, 4060, 28740, 28774, 28750, 14090, 1024, 6485, 261, 7590, 28723, 13, 1014, 4372, 349, 28705, 28770, 28784\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids), len(prompt_ids))\n",
    "idx = len(prompt_ids)\n",
    "print(', '.join([str(x) for x in input_ids[:idx]]))\n",
    "print(', '.join([str(x) for x in prompt_ids]))\n",
    "print(', '.join([str(x) for x in input_ids[idx:]]))\n",
    "print(', '.join([str(x) for x in output_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-QUESTION: How much less does Andy weigh now than at the beginning of the year?\n",
      "\n",
      "ANSWER:-\n",
      "-Andy weighed 156 + 36 = <<156+36=192>>192 pounds after growing taller.\n",
      "The answer is 36-\n",
      "-QUESTION: How much less does Andy weigh now than at the beginning of the year?\n",
      "\n",
      "ANSWER: Andy weighed 156 + 36 = <<156+36=192>>192 pounds after growing taller.\n",
      "The answer is 36-\n",
      "-Andy weighed 156 + 36 = <<156+36=192>>192 pounds after growing taller.\n",
      "The answer is 36-\n"
     ]
    }
   ],
   "source": [
    "print('-{}-'.format(tokenizer.decode(input_ids[:idx], clean_up_tokenization_spaces=False, skip_special_tokens=False)))\n",
    "print('-{}-'.format(tokenizer.decode(input_ids[idx:], clean_up_tokenization_spaces=False, skip_special_tokens=False)))\n",
    "print('-{}-'.format(tokenizer.decode(input_ids, clean_up_tokenization_spaces=False, skip_special_tokens=False)))\n",
    "print('-{}-'.format(tokenizer.decode(output_ids, clean_up_tokenization_spaces=False, skip_special_tokens=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'safe-rlhf-copy (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from string import punctuation\n",
    "\n",
    "def load_jsonl(fname):\n",
    "    with jsonlines.open(fname, mode='r') as reader:\n",
    "        data = [l for l in reader]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'safe-rlhf-copy (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from mcts_rl.utils import math_equal, extract_answer    #, extract_answer\n",
    "from mcts_rl.configs.constants import COT_INSTRUCTIONS, PROMPT_BEGIN, PROMPT_ASSISTANT, PROMPT_USER\n",
    "\n",
    "PROMPT_USER: str = '<|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|>'\n",
    "PROMPT_ASSISTANT: str = '<|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "EVAL_PROMPT_USER: str = '<|start_header_id|>user<|end_header_id|>\\n\\n'\n",
    "EVAL_PROMPT_ASSISTANT: str = '<|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "PROMPT_ASSISTANT_MCQ: str = ''\n",
    "\n",
    "def extract_pred_result(raw_pred, dtype='default'):\n",
    "    predictions, lens = {}, []\n",
    "    for dt in raw_pred:\n",
    "        prompt = dt['prompt'][0].strip().replace(PROMPT_BEGIN, '').replace(PROMPT_USER, '').replace(PROMPT_ASSISTANT, '').strip()\n",
    "        if dtype == 'mcts':\n",
    "            generated = dt['generated'][-1][-1] if len(dt['generated']) else None\n",
    "        elif dtype == 'llama3':\n",
    "            generated = dt['generated'][0].split('assistant')[0].strip()#.replace('I hope it is correct.', '').strip()\n",
    "        else:\n",
    "            generated = dt['generated'][0] if len(dt['generated']) == 1 else dt['generated']\n",
    "        lens.append(len(dt['generated']))\n",
    "        gt_answer = (dt['answer'], dt['answer_content'],)\n",
    "        if prompt in predictions: continue\n",
    "        predictions[prompt] = {'pred': generated, 'gt_answer': gt_answer}\n",
    "    avg_lens = sum(lens) / len(lens)\n",
    "    if avg_lens > 1:\n",
    "        lens.sort()\n",
    "        print(avg_lens, lens[len(lens)//2])\n",
    "    return predictions\n",
    "\n",
    "def extract_sc_answer(gens, use_code=False, k=100):\n",
    "    preds = [extract_answer(g, use_code=use_code) for g in gens[:k]]\n",
    "    counter = defaultdict(int)\n",
    "    for p in preds:\n",
    "        counter[p] += 1\n",
    "    return max(counter.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "def visualize_pred_result(predictions, N=int(1e5), use_code=False, k=100):\n",
    "    accu, errors, idx = [], [], -1\n",
    "    for prompt, gens in predictions.items():\n",
    "        idx += 1\n",
    "        sft_gen = gens['pred']\n",
    "        pred = extract_answer(sft_gen, use_code=use_code) if isinstance(sft_gen, str) else extract_sc_answer(sft_gen, use_code=use_code, k=k)\n",
    "        correct = math_equal(pred, gens['gt_answer'][0])\n",
    "        if '\\nAnswer Choices: ' in prompt:\n",
    "            options = prompt.split('\\nAnswer Choices: ')[-1].replace('Write a Python program to solve this.', '').strip()\n",
    "            option = options.split(f\"({gens['gt_answer'][0]})\")[-1].split(' (')[0].strip()\n",
    "            correct = correct or (math_equal(pred, option))\n",
    "            gens['option_content'] = option\n",
    "        gens['index'] = idx\n",
    "        gens['prompt'] = prompt\n",
    "        gens['pred_rst'] = pred\n",
    "        gens['correct'] = correct\n",
    "        errors.append(gens)\n",
    "        accu.append(correct)\n",
    "\n",
    "    print('all', sum(accu[:N])/max(1, len(accu[:N])), '({})'.format(len(accu[:N])))\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'safe-rlhf-copy (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "N = 9330\n",
    "model_name = 'cdpo-fullsft'\n",
    "print(f'=== {model_name} ===')\n",
    "for stp in [1792, 2688, 3712]:\n",
    "    rst2 = visualize_pred_result(\n",
    "        extract_pred_result(\n",
    "            load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/checkpoints/arithmetic/predictions/mistral_{model_name}_math{stp}.jsonl'),\n",
    "        ),\n",
    "        N=N,\n",
    "    )\n",
    "model_name = 'cdpo-fullsft-it16'\n",
    "print(f'=== {model_name} ===')\n",
    "for stp in [1920, 2816]:\n",
    "    rst2 = visualize_pred_result(\n",
    "        extract_pred_result(\n",
    "            load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/checkpoints/arithmetic/predictions/mistral_{model_name}_math{stp}.jsonl'),\n",
    "        ),\n",
    "        N=N,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'safe-rlhf-copy (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "N = 4410\n",
    "model_name = 'cdpo-fullsft'\n",
    "print(f'=== {model_name} ===')\n",
    "for stp in [1792, 2688, 3712]:\n",
    "    rst2 = visualize_pred_result(\n",
    "        extract_pred_result(\n",
    "            load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/checkpoints/arithmetic/predictions/mistral_{model_name}_gsm{stp}.jsonl'),\n",
    "        ),\n",
    "        N=N,\n",
    "    )\n",
    "model_name = 'cdpo-fullsft-it16'\n",
    "print(f'=== {model_name} ===')\n",
    "for stp in [1920, 2816]:\n",
    "    rst2 = visualize_pred_result(\n",
    "        extract_pred_result(\n",
    "            load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/checkpoints/arithmetic/predictions/mistral_{model_name}_gsm{stp}.jsonl'),\n",
    "        ),\n",
    "        N=N,\n",
    "    )\n",
    "print('=== baseline ===')\n",
    "rst2 = visualize_pred_result(\n",
    "    extract_pred_result(\n",
    "        load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/sft/diymistral-arithmo-lowerlr/predictions/gsm-cot.jsonl'),\n",
    "    ),\n",
    "    N=N,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'safe-rlhf-copy (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import regex\n",
    "import jsonlines\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "def load_jsonl(fname):\n",
    "    with jsonlines.open(fname, mode='r') as reader:\n",
    "        data = [l for l in reader]\n",
    "    return data\n",
    "\n",
    "def _extract_answer(gen):\n",
    "    raw_gen = gen\n",
    "    format_flag = False\n",
    "    if 'QUESTION:' in gen:\n",
    "        gen = gen.split('QUESTION:')[0]\n",
    "    if ' answer is' in gen:\n",
    "        gen = gen.strip().split(' answer is')[-1].strip().strip(punctuation).strip()\n",
    "        format_flag = True\n",
    "    elif 'Overall, ' in gen:\n",
    "        gen = gen.strip().split('Overall, ')[-1].strip().strip(punctuation).strip()\n",
    "        format_flag = True\n",
    "    elif 'Answer: ' in gen:\n",
    "        gen = gen.strip().split('Answer: ')[-1].strip().strip(punctuation).strip()\n",
    "        format_flag = True\n",
    "    elif 'Therefore, ' in gen:\n",
    "        gen = gen.strip().split('Therefore, ')[-1].strip().strip(punctuation).strip()\n",
    "        format_flag = True\n",
    "    elif 'Thus, ' in gen:\n",
    "        gen = gen.strip().split('Thus, ')[-1].strip().strip(punctuation).strip()\n",
    "        format_flag = True\n",
    "    elif 'So, ' in gen:\n",
    "        gen = gen.strip().split('So, ')[-1].strip().strip(punctuation).strip()\n",
    "        format_flag = True\n",
    "    elif 'So ' in gen:\n",
    "        gen = gen.strip().split('So ')[-1].strip().strip(punctuation).strip()\n",
    "        format_flag = True\n",
    "    elif '\\n' in gen:\n",
    "        gen = gen.strip().split('\\n')[-1].strip().strip(punctuation).strip()\n",
    "    \n",
    "    options = regex.findall(r'\\([A-Z1-9]\\)|[A-Z1-9]\\)', gen)\n",
    "    options_backup = regex.findall(r'[A-Z1-9]', gen)\n",
    "    if options:\n",
    "        options = [x for i, x in enumerate(options) if x not in options[i+1:]]\n",
    "        prediction = options[-1].strip(punctuation)\n",
    "    elif options_backup and format_flag:\n",
    "        options = [x for i, x in enumerate(options_backup) if x not in options_backup[i+1:]]\n",
    "        prediction = options[-1].strip(punctuation)\n",
    "    else:\n",
    "        options = regex.findall(r'\\([A-Z1-9]\\)|[A-Z1-9]\\)', raw_gen)\n",
    "        if options:\n",
    "            options = [x for i, x in enumerate(options) if x not in options[i+1:]]\n",
    "            prediction = options[-1].strip(punctuation)\n",
    "        else:\n",
    "            prediction = None\n",
    "    return prediction\n",
    "\n",
    "def eval_accu(prediction, option, answer):    \n",
    "    if isinstance(prediction, str):\n",
    "        return _extract_answer(prediction) == option.strip(punctuation) or answer in prediction.strip().split('\\n')[-1]\n",
    "    else:\n",
    "        counter = defaultdict(int)\n",
    "        for g in prediction[:30]:\n",
    "            g = _extract_answer(g)\n",
    "            if g is not None:\n",
    "                counter[g] += 1\n",
    "        try:\n",
    "            return max(counter.items(), key=lambda x: x[1])[0] == option.strip(punctuation)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "def load_raw_labels(dtype, valtest=False):\n",
    "    if valtest:\n",
    "        raw_labels = load_jsonl(f'/home/users/nus/e0672129/scratch/csr/mcq_{dtype}_test.jsonl')\n",
    "    else:\n",
    "        # raw_labels = load_jsonl(f'/home/users/nus/e0672129/scratch/csr/mcq_{dtype}_fulltest.jsonl')\n",
    "        # raw_labels = load_jsonl(f'/home/users/nus/e0672129/scratch/csr/mcq_test.jsonl')\n",
    "        raw_labels = load_jsonl(f'/home/users/nus/e0672129/scratch/csr/mcq_test.jsonl')\n",
    "    raw_labels_dict = {}\n",
    "    for i, dt in enumerate(raw_labels):\n",
    "        qu = (dt['question'].strip(), dt['answer'])\n",
    "        raw_labels_dict[qu] = dt.get('label', 'csqa')\n",
    "    return raw_labels_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'safe-rlhf-copy (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from mcts_rl.configs.constants import COT_INSTRUCTIONS, PROMPT_BEGIN, PROMPT_ASSISTANT, PROMPT_USER, SQA_PROMPT\n",
    "\n",
    "PROMPT_ASSISTANT: str = 'ANSWER:'\n",
    "\n",
    "SQA_PROMPT = SQA_PROMPT.replace('</s>\\n\\n', ' ').replace('</s>', '').strip()\n",
    "\n",
    "def extract_pred_result(raw_preds, dtype='default'):\n",
    "    predictions, lens = {}, []\n",
    "    if len(raw_preds) > 5:\n",
    "        raw_preds = [raw_preds]\n",
    "    for raw_pred in raw_preds:\n",
    "        for dt in raw_pred:\n",
    "            prompt = dt['prompt'][0].strip().replace(SQA_PROMPT, '').strip().replace(PROMPT_BEGIN, '').replace(PROMPT_USER, '').split(PROMPT_ASSISTANT)[0].strip()\n",
    "            if dtype == 'fewshot':\n",
    "                prompt = dt['prompt'][0].strip().split('QUESTION:')[-1].strip().strip().replace(SQA_PROMPT, '').strip().replace(PROMPT_BEGIN, '').replace(PROMPT_USER, '').split(PROMPT_ASSISTANT)[0].strip()\n",
    "            if dtype == 'mcts':\n",
    "                generated = dt['generated'][-1][-1] if len(dt['generated']) else None\n",
    "            elif dtype == 'fewshot':\n",
    "                generated = dt['generated'][0].split('QUESTION')[0].strip()\n",
    "            else:\n",
    "                generated = dt['generated'][0] if len(dt['generated']) == 1 else dt['generated']\n",
    "            lens.append(len(dt['generated']))\n",
    "            gt_answer = (dt['answer'], dt['answer_content'],)\n",
    "            if prompt not in predictions:\n",
    "                predictions[prompt] = {'pred': generated, 'gt_answer': gt_answer}\n",
    "            elif isinstance(predictions[prompt]['pred'], list):\n",
    "                predictions[prompt]['pred'] += generated\n",
    "    return predictions\n",
    "\n",
    "def visualize_pred_result(predictions, N=int(1e5), dtype='csr', show_split=False, valtest=False):\n",
    "    raw_labels_dict = load_raw_labels(dtype, valtest=valtest)\n",
    "    accu = []\n",
    "    tsk_accu = {}\n",
    "    if dtype == 'sqa':\n",
    "        tsk_accu = {x:[] for x in ['openbook', 'arc_easy', 'arc_hard', 'ai2s_ele', 'ai2s_mid', 'sciq']}\n",
    "    elif dtype == 'csqa':\n",
    "        tsk_accu = {x:[] for x in ['csqa', 'sciq', 'arc_hard']}\n",
    "        # tsk_accu = {x:[] for x in ['csqa', 'openbook', 'arc_easy', 'arc_hard', 'ai2s_ele', 'ai2s_mid', 'sciq']}\n",
    "    for prompt, gens in predictions.items():\n",
    "        sft_gen = gens['pred']\n",
    "        _eval = eval_accu(sft_gen, gens['gt_answer'][0], gens['gt_answer'][1])\n",
    "        tsk = prompt.replace('QUESTION:', '').strip().split(PROMPT_ASSISTANT)[0].strip()\n",
    "        tsk = (tsk, gens['gt_answer'][0])\n",
    "        if tsk not in raw_labels_dict:\n",
    "            continue\n",
    "        if tsk_accu is not None and raw_labels_dict[tsk] not in tsk_accu:\n",
    "            continue\n",
    "        accu.append(_eval)\n",
    "        if tsk_accu is not None:\n",
    "            tsk_accu[raw_labels_dict[tsk]].append(_eval)\n",
    "        if len(accu) >= N:\n",
    "            break\n",
    "\n",
    "    print('* all', sum(accu)/max(1, len(accu)), '({})'.format(len(accu)))\n",
    "    if not show_split or tsk_accu is None:\n",
    "        return\n",
    "    for k, v in tsk_accu.items():\n",
    "        print(k, sum(v)/max(1, len(v)), '({})'.format(len(v)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'safe-rlhf-copy (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "N = 23910\n",
    "show_split=False\n",
    "model_name = 'cdpo-4x2-it32'\n",
    "print(f'=== {model_name} ===')\n",
    "for stp in [1024, 1152, 2048]:\n",
    "    visualize_pred_result(\n",
    "        extract_pred_result(\n",
    "            load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/checkpoints/sqa/predictions/mistral_{model_name}_{stp}.jsonl'),\n",
    "        ),\n",
    "        dtype='sqa',\n",
    "        N=N,\n",
    "        show_split=show_split,\n",
    "    )\n",
    "model_name = 'cdpo-4x2-bs32'\n",
    "print(f'=== {model_name} ===')\n",
    "for stp in [1024]:\n",
    "    visualize_pred_result(\n",
    "        extract_pred_result(\n",
    "            load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/checkpoints/sqa/predictions/mistral_{model_name}_{stp}.jsonl'),\n",
    "        ),\n",
    "        dtype='sqa',\n",
    "        N=N,\n",
    "        show_split=show_split,\n",
    "    )\n",
    "model_name = 'cdpo-4x2'\n",
    "print(f'=== {model_name} ===')\n",
    "for stp in [1024]:\n",
    "    visualize_pred_result(\n",
    "        extract_pred_result(\n",
    "            load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/checkpoints/sqa/predictions/mistral_{model_name}_{stp}.jsonl'),\n",
    "        ),\n",
    "        dtype='sqa',\n",
    "        N=N,\n",
    "        show_split=show_split,\n",
    "    )\n",
    "model_name = 'cdpo-3x2'\n",
    "print(f'=== {model_name} ===')\n",
    "for stp in [1024, 2048]:\n",
    "    visualize_pred_result(\n",
    "        extract_pred_result(\n",
    "            load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/checkpoints/sqa/predictions/mistral_{model_name}_{stp}.jsonl'),\n",
    "        ),\n",
    "        dtype='sqa',\n",
    "        N=N,\n",
    "        show_split=show_split,\n",
    "    )\n",
    "model_name = 'cdpo-3x3'\n",
    "print(f'=== {model_name} ===')\n",
    "for stp in [768, 1024]:\n",
    "    visualize_pred_result(\n",
    "        extract_pred_result(\n",
    "            load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/checkpoints/sqa/predictions/mistral_{model_name}_{stp}.jsonl'),\n",
    "        ),\n",
    "        dtype='sqa',\n",
    "        N=N,\n",
    "        show_split=show_split,\n",
    "    )\n",
    "print('=== baseline ===')\n",
    "visualize_pred_result(\n",
    "    extract_pred_result(\n",
    "        load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/experiments/sqa/predictions/mistral_baseline.jsonl'),\n",
    "    ),\n",
    "    dtype='sqa',\n",
    "    N=N,\n",
    "    show_split=show_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* all 0.3145539906103286 (213)\n",
      "csqa 0.3145539906103286 (213)\n",
      "sciq 0.0 (0)\n",
      "arc_hard 0.0 (0)\n",
      "* all 0.37089201877934275 (213)\n",
      "csqa 0.37089201877934275 (213)\n",
      "sciq 0.0 (0)\n",
      "arc_hard 0.0 (0)\n"
     ]
    }
   ],
   "source": [
    "N = 213\n",
    "visualize_pred_result(\n",
    "    extract_pred_result(\n",
    "        load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/experiments/csr/predictions/csqa/gptj_cdpo_instance_1024.jsonl'),\n",
    "        dtype='fewshot',\n",
    "    ),\n",
    "    dtype='csqa',\n",
    "    N=N,\n",
    "    show_split=True,\n",
    "    valtest=True,\n",
    ")\n",
    "visualize_pred_result(\n",
    "    extract_pred_result(\n",
    "        load_jsonl(f'/home/users/nus/e0672129/scratch/MCTS-DPO/outputs/experiments/csr/predictions/csqa/gptj_cdpo_step_1024.jsonl'),\n",
    "        dtype='fewshot',\n",
    "    ),\n",
    "    dtype='csqa',\n",
    "    N=N,\n",
    "    show_split=True,\n",
    "    valtest=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
